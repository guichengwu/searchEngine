{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/qda.py:4: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.85      1.00      0.92      1998\n",
      "  Relevance       1.00      0.19      0.32       422\n",
      "\n",
      "avg / total       0.88      0.86      0.82      2420\n",
      "\n",
      "knn\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.99      1.00      0.99      1998\n",
      "  Relevance       0.99      0.94      0.96       422\n",
      "\n",
      "avg / total       0.99      0.99      0.99      2420\n",
      "\n",
      "linear support vector machine\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.89      1.00      0.94      1998\n",
      "  Relevance       1.00      0.41      0.58       422\n",
      "\n",
      "avg / total       0.91      0.90      0.88      2420\n",
      "\n",
      "bayes gaussian\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.99      1.00      0.99      1998\n",
      "  Relevance       0.98      0.94      0.96       422\n",
      "\n",
      "avg / total       0.99      0.99      0.99      2420\n",
      "\n",
      "logistic regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.86      1.00      0.93      1998\n",
      "  Relevance       1.00      0.25      0.40       422\n",
      "\n",
      "avg / total       0.89      0.87      0.83      2420\n",
      "\n",
      "LDA\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.89      1.00      0.94      1998\n",
      "  Relevance       0.98      0.40      0.57       422\n",
      "\n",
      "avg / total       0.90      0.89      0.87      2420\n",
      "\n",
      "lasso\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.83      1.00      0.90      1998\n",
      "  Relevance       0.00      0.00      0.00       422\n",
      "\n",
      "avg / total       0.68      0.83      0.75      2420\n",
      "\n",
      "decision tree\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "Irrevelence       0.99      1.00      0.99      1998\n",
      "  Relevance       0.99      0.94      0.96       422\n",
      "\n",
      "avg / total       0.99      0.99      0.99      2420\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/svm/base.py:514: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y_ = column_or_1d(y, warn=True)\n",
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/guichengwu/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Web page 1: http://www.warwickdc-leisure.co.uk/sports_dev.htm',\n",
       " 'Web page 2: http://www.miami.com/mld/miamiherald/sports/high_school/4606722.htm',\n",
       " 'Page Relevance Score: 0.0869565217391']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################################\n",
    "##  ECS 251 Spring 2016 Final Project Code\n",
    "##  Guicheng Wu\n",
    "##  Xingtai Li\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib import urlopen\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "import pylab\n",
    "import copy\n",
    "import random\n",
    "import difflib\n",
    "from sklearn import svm, naive_bayes,linear_model, neighbors, preprocessing, cross_validation,decomposition,tree\n",
    "from sklearn.metrics.pairwise import rbf_kernel,sigmoid_kernel,laplacian_kernel,safe_sparse_dot,check_pairwise_arrays\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from sklearn.utils.extmath import row_norms, safe_sparse_dot\n",
    "import math\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,hamming_loss\n",
    "\n",
    "############################################################################\n",
    "# Create a search engine object\n",
    "class se:\n",
    "    def __init__(self,adjacent_list,node_list,key):\n",
    "        self.G = self.read_links(adjacent_list)\n",
    "        self.N = self.read_nodes(node_list)\n",
    "        self.G.remove_node(-1)\n",
    "        #self.delete_empty()\n",
    "        self.keyword = key\n",
    "        self.directory_text = 'basketball/'\n",
    "        # write the webpage content to text files\n",
    "        #directory = 'basketball/'\n",
    "        #write_webpage_txt(directory,self.__N)\n",
    "        self.tfidf,self.countvect,self.X = self.TFIDF_weight()\n",
    "        self.tfidf_key = self.TFIDF_keyword()\n",
    "        self.pr = self.page_rank(0.9).values()\n",
    "        self.hub_weights = self.hits()[0].values()\n",
    "        self.authorities_weights = self.hits()[1].values()\n",
    "        self.title_count = self.title_key()\n",
    "        self.construct_feature_matrix()\n",
    "        \n",
    "    # Construct the link graph\n",
    "    def read_links(self,name):\n",
    "        # read adjacent list file\n",
    "        with open(name) as f:\n",
    "            lists = []\n",
    "            for line in f:\n",
    "                lists.append(line.replace(\":\",\"\").rstrip())\n",
    "        # Form graph\n",
    "        G = nx.parse_adjlist(lists,create_using=nx.DiGraph(),nodetype=int)\n",
    "        return G\n",
    "\n",
    "    # Read the node file and construct a dictionary of nodes\n",
    "    def read_nodes(self,name):\n",
    "        with open(name) as f:\n",
    "            number_of_node = int(next(f).rstrip())\n",
    "            next(f)\n",
    "            D = {}\n",
    "            for i in range(number_of_node):\n",
    "                d = {}\n",
    "                idd = int(next(f).split(' ')[0])\n",
    "                d['url'] = next(f).rstrip()\n",
    "                d['title'] = next(f).rstrip()\n",
    "                links = next(f).rstrip().split(' ')\n",
    "                d['in'] = int(links[0])\n",
    "                d['out'] = int(links[1])\n",
    "                D[idd] = d\n",
    "                if i != number_of_node - 1:\n",
    "                    next(f)\n",
    "        return D\n",
    "    \n",
    "    # Get the page rank scores\n",
    "    def page_rank(self,alpha):\n",
    "        return nx.pagerank(self.G, alpha=alpha)\n",
    "\n",
    "    # Get the hits scores\n",
    "    def hits(self):\n",
    "        return nx.hits(self.G)\n",
    "\n",
    "    # Number of terms in the title of page\n",
    "    def title_key(self):\n",
    "        \n",
    "        title_keys = []\n",
    "        for key in self.N:\n",
    "            title_words = self.N[key]['title'].split()\n",
    "            http = self.N[key]['url'].split()\n",
    "            matching = [s for s in title_words if self.keyword in s]\n",
    "            matching_2 = [s for s in http if self.keyword in s]\n",
    "            title_keys.append(len(matching)+len(matching_2))\n",
    "#         title_keys = np.matrix(title_keys,dtype='float')\n",
    "#         return preprocessing.MinMaxScaler().fit_transform(title_keys)\n",
    "        return title_keys\n",
    "        \n",
    "    # Read the webpage links\n",
    "    def retrive_html_content(url):\n",
    "        try:\n",
    "            html = urlopen(url).read()\n",
    "        except:\n",
    "            logging.error('Data cannot retrieved in the \\nURL: %s',url)\n",
    "            html = \"\"  \n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        return text\n",
    "\n",
    "    # Write the web content to the specified directory\n",
    "    def write_webpage_txt(self,directory):\n",
    "        for i in range(3527,len(N)):\n",
    "            link = N[i]['url']\n",
    "            page_content = retrive_html_content(link).encode(\"utf8\")\n",
    "            web_file =directory +  str(i) + '.txt'\n",
    "            file_stream = open(web_file, 'w')\n",
    "            file_stream.write(page_content)\n",
    "\n",
    "    #build TDIDF matrix\n",
    "    def TFIDF_weight(self):\n",
    "        corpus = []\n",
    "        webpage_count = len(self.N)\n",
    "        for i in range(webpage_count):\n",
    "            web_file = self.directory_text + str(i) + '.txt'\n",
    "            file_stream = open(web_file, 'r')\n",
    "            content = file_stream.read()\n",
    "            corpus.append(content)\n",
    "\n",
    "        vectorizer = CountVectorizer(min_df=1)\n",
    "        countvect = vectorizer.fit(corpus)\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        transformer = TfidfTransformer()\n",
    "        #build TFIDF matrix\n",
    "        tfidf = transformer.fit_transform(X)\n",
    "        return tfidf,countvect,X\n",
    "\n",
    "    # Calculate the weight of tfidf \n",
    "    def TFIDF_keyword(self):\n",
    "        #get all features\n",
    "        words = self.countvect.get_feature_names()\n",
    "        #if keywords exist in features, get the TFIDF weight; else set TFIDF weight as 0\n",
    "        tf_new = np.zeros((len(self.G),1))\n",
    "        list1 = []\n",
    "        for i in range(self.tfidf.shape[1]):\n",
    "            if self.keyword in words[i]:\n",
    "                list1.append(i)\n",
    "                tf_new+=self.tfidf[:,i]\n",
    "        \n",
    "        return tf_new\n",
    "    \n",
    "    # Construct the feature matrix \n",
    "    def construct_feature_matrix(self):\n",
    "        feature_matrix = np.column_stack((self.pr,self.hub_weights,self.authorities_weights,self.title_count,self.tfidf_key))\n",
    "        feature_file = self.keyword+'.txt'\n",
    "        np.savetxt(feature_file,feature_matrix)\n",
    "        \n",
    "    # Retrieve the top words from the tfidf\n",
    "    def retrieve_top_words(self,size):\n",
    "        words = self.c_1.get_feature_names()\n",
    "        word_count = ((self.X.sum(axis=0).T).tolist())\n",
    "        indicies = [i[0] for i in sorted(enumerate(word_count), key=lambda x:x[1])]\n",
    "        top_indicies = indicies[len(word_count)-size:len(word_count)]\n",
    "        top_words = []\n",
    "        for i in top_indicies:\n",
    "            top_words.append(words[i])\n",
    "        return top_words\n",
    "    \n",
    "    # Get the prediction score from the tfidf_key\n",
    "    def get_prediction(self,key_list):\n",
    "        names = self.countvect.get_feature_names()\n",
    "        y_pred = np.zeros((len(self.N),len(key_list)))\n",
    "        for i in range(self.X.shape[1]):\n",
    "            for j in range(len(key_list)):\n",
    "                if key_list[j] in names[i]:\n",
    "                    for k in range(len(self.N)):\n",
    "                        if y_pred[k][j] == 0:\n",
    "                            y_pred[k][j] = 1\n",
    "        return y_pred\n",
    "    \n",
    "    # Label the response\n",
    "    def label_title(self):\n",
    "        label_y = []\n",
    "        i = 0\n",
    "        for key in self.N:\n",
    "            title_words = self.N[key]['title'].split()\n",
    "            if self.keyword in title_words:\n",
    "                label_y.append(1)\n",
    "            else:\n",
    "                label_y.append(0)\n",
    "            i += 1 \n",
    "        label_y = np.matrix(label_y).T\n",
    "        return label_y\n",
    "    \n",
    "###################################################\n",
    "## Label y\n",
    "def label_y(engine,num):\n",
    "    count = []\n",
    "    names = engine.countvect.get_feature_names()\n",
    "    for i in range(engine.X.shape[1]):\n",
    "        if engine.keyword in names[i]:\n",
    "            count.append(i)\n",
    "    V = engine.X\n",
    "    y_news = np.zeros((len(engine.G),1))\n",
    "    for i in range(len(count)):\n",
    "        y_news += V[:,count[i]]\n",
    "\n",
    "    for i in range(len(y_news)):\n",
    "        if y_news[i] > num:\n",
    "            y_news[i] = 1\n",
    "        else:\n",
    "            y_news[i] = 0\n",
    "    y_label = engine.label_title()\n",
    "    y_news += y_label\n",
    "    cc = 0\n",
    "    for i in range(len(y_news)):\n",
    "        if y_news[i] != 0:\n",
    "            cc+=1\n",
    "            y_news[i] = 1\n",
    "    np.savetxt('ncaa_y1.txt',y_news,fmt='%i')\n",
    "    return cc\n",
    "\n",
    "##########################################################################\n",
    "## Create a ranking object which return a list of hyperlink according to their revelance to query\n",
    "class ranking():\n",
    "    def __init__(self,q,engine):\n",
    "        self.q = q\n",
    "        self.engine = engine\n",
    "        self.feature = self.get_feature()\n",
    "        self.label = self.get_label()\n",
    "        self.G, self.N, self.index,self.feature_t = self.create_graph()\n",
    "        self.score = self.ranking_score()\n",
    "        \n",
    "    # Get feature matrix\n",
    "    def get_feature(self):\n",
    "        with open('dataset/'+self.q+'.txt') as f:\n",
    "            feature = []\n",
    "            for line in f:\n",
    "                feature.append(line.rstrip().split(' '))\n",
    "            feature = np.array(feature,dtype='float')\n",
    "        return feature\n",
    "    # Get label of pages\n",
    "    def get_label(self):\n",
    "        with open('dataset/'+self.q+'_y2.txt') as f:\n",
    "            label = []\n",
    "            for line in f:\n",
    "                label.append(int(line.rstrip()))\n",
    "        return label\n",
    "    # Create Gragh\n",
    "    def create_graph(self):\n",
    "        index_d = []\n",
    "        index_k = []\n",
    "        N = self.engine.N\n",
    "        G = self.engine.G\n",
    "        feature_t = []\n",
    "        for i in range(len(self.label)):\n",
    "            if self.label[i] == 0:\n",
    "                index_d.append(i)\n",
    "                del N[i]\n",
    "            else:\n",
    "                index_k.append(i)\n",
    "                feature_t.append(np.append((i),self.feature[i]))\n",
    "        G.remove_nodes_from(index_d)\n",
    "        return G,N,index_k,np.array(feature_t)\n",
    "    # Print the ranking of hyperlinks\n",
    "    def ranking_score(self):\n",
    "        result = []\n",
    "\n",
    "        rank = np.zeros((len(self.N),7))\n",
    "        rank[:,0] = self.feature_t[:,0]\n",
    "        \n",
    "        num_pr = np.argsort(self.feature_t[:,1])\n",
    "        num_aut = np.argsort(self.feature_t[:,2])\n",
    "        num_hub = np.argsort(self.feature_t[:,3])\n",
    "        num_title = np.argsort(self.feature_t[:,4])\n",
    "        num_content = np.argsort(self.feature_t[:,5])\n",
    "        \n",
    "        for j in range(len(num_title)):\n",
    "            rank[num_pr[j]][1] = j+1\n",
    "            rank[num_aut[j]][2] = j+1\n",
    "            rank[num_hub[j]][3] = j+1\n",
    "            rank[num_title[j]][4] = j+1\n",
    "            rank[num_content[j]][5] = j+1\n",
    "        \n",
    "        tnum = rank.shape[0]\n",
    "        for k in range(rank.shape[0]):\n",
    "            rank[k][6] = np.multiply(10,[(tnum-rank[k][1])/tnum])+np.multiply(10,[(tnum-rank[k][2])/tnum])\n",
    "            +np.multiply(10,[(tnum-rank[k][3])/tnum])+np.multiply(100,[(tnum-rank[k][4])/tnum])\n",
    "            +np.multiply(10,[(tnum-rank[k][5])/tnum])\n",
    "        indexes = []\n",
    "        for key in self.N:\n",
    "            if self.q in self.N[key]['url']:\n",
    "                indexes.append(key)\n",
    "        for u in range(len(indexes)):\n",
    "            for o in range(rank.shape[0]):\n",
    "                if rank[o][0] == indexes[u]:\n",
    "                    rank[o][6] = np.multiply(10,rank[o][6])        \n",
    "        final = np.argsort(rank[:,6])\n",
    "        for i in range(tnum):\n",
    "            result.append(self.N[self.feature_t[final[i],0]]['url'])\n",
    "        return result\n",
    "    # Save the txt file    \n",
    "    def print_score(self):\n",
    "        np.savetxt('result/'+self.q+ '_score.txt',self.score,fmt=\"%s\")\n",
    "\n",
    "###################################################################################\n",
    "def generate_ranking_scores():\n",
    "    for i in range(len(query)):\n",
    "        engine = se('All/basketball/graph/adj_list','All/basketball/graph/nodes',query[i])\n",
    "        r = ranking(query[i],engine)\n",
    "        #r.print_score()\n",
    "        #save_graph(r.G,'result/'+query[i]+'.png')\n",
    "        \n",
    "###################################################################################\n",
    "# draw the graph\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(20, 20), dpi=80)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos)\n",
    "    nx.draw_networkx_edges(graph,pos)\n",
    "    nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "    cut = 1.00\n",
    "    xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    plt.xlim(0, xmax)\n",
    "    plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "    pylab.close()\n",
    "    del fig\n",
    "#save_graph(r.G,'my_graph_1.pdf')\n",
    "\n",
    "###################################################################################\n",
    "## Calculate page relevance \n",
    "class page_relevance():\n",
    "    \n",
    "    def __init__(self,engine):\n",
    "        self.engine = engine\n",
    "        self.page_1, self.page_2 = self.get_pages()\n",
    "        self.score = self.check()\n",
    "        #self.print_scores()\n",
    "        #self.print_example()\n",
    "    # Randomly generate two pages\n",
    "    def get_pages(self):\n",
    "        page = random.sample(self.engine.N,2)\n",
    "        return page[0],page[1]\n",
    "    # Check the decision tree, return a score of two pages\n",
    "    def check(self):\n",
    "        if nx.has_path(self.engine.G,self.page_1,self.page_2):\n",
    "            length,path = nx.bidirectional_dijkstra(self.engine.G,self.page_1,self.page_2)\n",
    "        else:\n",
    "            length = 0\n",
    "        value = 0\n",
    "        score_2 = self.text_match()\n",
    "        if length > 0 & length < 10:\n",
    "            value = (11.0-float(length))/10.0\n",
    "        elif length >= 10:\n",
    "            score_l = 1.0/float(length)\n",
    "            value = score_1 + score_2\n",
    "        else:\n",
    "            value = score_2\n",
    "        return value\n",
    "    # Texutal Relevance       \n",
    "    def text_match(self):\n",
    "        score = 0.0\n",
    "        web_1 = self.engine.N[self.page_1]['url'][7:-5]\n",
    "        web_2 = self.engine.N[self.page_2]['url'][7:-5]\n",
    "        title_1 = self.engine.N[self.page_1]['title']\n",
    "        title_2 = self.engine.N[self.page_2]['title']\n",
    "        file_name_1 = 'basketball/' + str(self.page_1) + '.txt'\n",
    "        file_stream = open(file_name_1, 'r')\n",
    "        content_1 = file_stream.read()\n",
    "        file_name_2 = 'basketball/' + str(self.page_2) + '.txt'\n",
    "        file_stream = open(file_name_2, 'r')\n",
    "        content_2 = file_stream.read()\n",
    "        \n",
    "        web_score = difflib.SequenceMatcher(None, web_1,web_2).ratio()\n",
    "        title_score = difflib.SequenceMatcher(None, title_1,title_2).ratio()\n",
    "        \n",
    "        if (web_score > 0.5) | (title_score > 0.5):\n",
    "            score = max(web_score,title_score)\n",
    "        else:\n",
    "            if (len(content_1) < 100) | (len(content_2)<100):\n",
    "                score = min(web_score,title_score)\n",
    "            else:\n",
    "                corpus = []\n",
    "                corpus.append(content_1)\n",
    "                corpus.append(content_2)\n",
    "                vectorizer = CountVectorizer(min_df=1)\n",
    "                X = vectorizer.fit_transform(corpus)\n",
    "                X = np.squeeze(np.asarray(X.todense()))\n",
    "                sort_1 = np.argsort(X[0])\n",
    "                sort_2 = np.argsort(X[1])\n",
    "                count_s = 0\n",
    "                same = []\n",
    "                for i in range(len(sort_1)):\n",
    "                    for j in range(len(sort_2)):\n",
    "                        if sort_1[i] ==  sort_2[j]:\n",
    "                            same.append(sort_1[i])\n",
    "                for k in range(len(same)):\n",
    "                    if (X[0][sort_1[same[k]]] > 1) & (X[1][sort_2[same[k]]] > 1):\n",
    "                        count_s += 1\n",
    "                count_score = float(count_s) / 10.0\n",
    "                score = min(web_score,title_score,count_s) \n",
    "        return score\n",
    "    # Good mathch example\n",
    "    def good_match(self):\n",
    "        web_1 = 'http://www.usatoday.com/sports/basketba/skm/acc/skma08.htm'\n",
    "        web_2 = 'http://www.usatoday.com/sports/basketba/skm/pac10/skmg09.htm'\n",
    "        web_3 = 'http://gostanford.fansonly.com/sports/w-baskbl/stats/teamstat.html'\n",
    "        web_4 = 'http://gostanford.fansonly.com/sports/w-baskbl/archive/stan-w-baskbl-archive.html'  \n",
    "        web_score_1 = difflib.SequenceMatcher(None, web_1,web_2).ratio()\n",
    "        web_score_2 = difflib.SequenceMatcher(None, web_3,web_4).ratio()\n",
    "        return web_score_1,web_1,web_2,web_score_2,web_3,web_4\n",
    "    # Good link example\n",
    "    def good_link(self):\n",
    "        length,path = nx.bidirectional_dijkstra(self.engine.G,0,195)\n",
    "        web_1 = self.engine.N[0]['url']\n",
    "        web_2 = self.engine.N[195]['url']\n",
    "        score = (11.0-float(length))/10.0 - (0.1 * difflib.SequenceMatcher(None, web_1,web_2).ratio())\n",
    "        return score,web_1,web_2\n",
    "    # Print the scores\n",
    "    def print_scores(self):\n",
    "        content = []\n",
    "        content.append('Web page 1: ' + self.engine.N[self.page_1]['url'])\n",
    "        content.append('Web page 2: ' + self.engine.N[self.page_2]['url'])\n",
    "        content.append('Page Relevance Score: ' + str(self.score))\n",
    "        return content\n",
    "    # Print example\n",
    "    def print_example(self):\n",
    "        content = []\n",
    "        good_m = self.good_match()\n",
    "        good_d = self.good_link()\n",
    "        content.append('Web page 1: ' + good_m[1])\n",
    "        content.append('Web page 2: ' + good_m[2])\n",
    "        content.append('Page Relevance Score: ' + str(good_m[0]))\n",
    "        content.append('')\n",
    "        content.append('Web page 1: ' + good_m[4])\n",
    "        content.append('Web page 2: ' + good_m[5])\n",
    "        content.append('Page Relevance Score: ' + str(good_m[3]))\n",
    "        content.append('')\n",
    "        content.append('Web page 1: ' + good_d[1])\n",
    "        content.append('Web page 2: ' + good_d[2])\n",
    "        content.append('Page Relevance Score: ' + str(good_d[0]))\n",
    "        return content\n",
    "\n",
    "##########################################################\n",
    "## Classification of page \n",
    "class classifier:\n",
    "    def decision_rule(self, hat_Y):\n",
    "        for i in np.arange(hat_Y.shape[0]):\n",
    "            if hat_Y[i] > 0.5:\n",
    "                hat_Y[i] = 1\n",
    "            else:\n",
    "                hat_Y[i] = 0\n",
    "        return hat_Y\n",
    "\n",
    "    def ridge_regression(self, alpha=10):\n",
    "        ridge = linear_model.Ridge(alpha)\n",
    "        ridge.fit(self.X_train, self.Y_train)\n",
    "        return ridge\n",
    "\n",
    "    def kNearestNeighbors(self, k=5):\n",
    "        knn = neighbors.KNeighborsRegressor(k)\n",
    "        knn.fit(self.X_train, self.Y_train)\n",
    "        return knn\n",
    "\n",
    "    #SVM kernel_type: linear, polynomial, rbf, sigmoid, p\n",
    "    def linear_svm(self):\n",
    "        linearsvm = svm.SVC(kernel='linear')\n",
    "        linearsvm.fit(self.X_train, self.Y_train)\n",
    "        return linearsvm\n",
    "    \n",
    "    def bayes_gaussian(self):\n",
    "        gaussianbayes = naive_bayes.GaussianNB()\n",
    "        gaussianbayes.fit(self.X_train, self.Y_train)\n",
    "        return gaussianbayes\n",
    "\n",
    "    def logistic_regression(self):\n",
    "        logistic = linear_model.LogisticRegression()\n",
    "        logistic.fit(self.X_train, self.Y_train)\n",
    "        return logistic\n",
    "\n",
    "    def linearDiscrminat(self):\n",
    "        linearDA = LDA()\n",
    "        linearDA.fit(self.X_train, self.Y_train)\n",
    "        return linearDA\n",
    "\n",
    "    def lasso_regression(self):\n",
    "        lasso_regr = linear_model.Lasso(alpha=1)\n",
    "        lasso_regr.fit(self.X_train, self.Y_train)\n",
    "        return lasso_regr\n",
    "\n",
    "    def decision_tree(self):\n",
    "        decisionTree = tree.DecisionTreeClassifier()\n",
    "        decisionTree.fit(self.X_train, self.Y_train)\n",
    "        return decisionTree\n",
    "\n",
    "    def predict_ridge_regression(self):\n",
    "        hat_Y = self.ridge_regression().predict(self.X_test)\n",
    "        return self.decision_rule(hat_Y)\n",
    "\n",
    "    def predict_knn(self, k=5):\n",
    "        hat_Y = self.kNearestNeighbors().predict(self.X_test)\n",
    "        return self.decision_rule(hat_Y)\n",
    "\n",
    "    def predict_linearsvm(self):\n",
    "        hat_Y = self.linear_svm().predict(self.X_test)\n",
    "        hat_Y = hat_Y.reshape(hat_Y.shape[0], 1)\n",
    "        return self.decision_rule(hat_Y)\n",
    "    \n",
    "    def predict_bayesgaussin(self):\n",
    "        hat_Y = self.bayes_gaussian().predict(self.X_test)\n",
    "        hat_Y = hat_Y.reshape(hat_Y.shape[0], 1)\n",
    "        return self.decision_rule(hat_Y)  \n",
    "\n",
    "    def predict_logistic(self):\n",
    "        hat_Y = self.logistic_regression().predict(self.X_test)\n",
    "        hat_Y = hat_Y.reshape(hat_Y.shape[0], 1)\n",
    "        return self.decision_rule(hat_Y)\n",
    "\n",
    "    def predict_LDA(self):\n",
    "        hat_Y = self.linearDiscrminat().predict(self.X_test)\n",
    "        hat_Y = hat_Y.reshape(hat_Y.shape[0], 1)\n",
    "        return self.decision_rule(hat_Y)\n",
    "\n",
    "    def predict_lasso(self):\n",
    "        hat_Y = self.lasso_regression().predict(self.X_test)\n",
    "        hat_Y = hat_Y.reshape(hat_Y.shape[0], 1)\n",
    "        return self.decision_rule(hat_Y)\n",
    "\n",
    "    def predict_decisionTree(self):\n",
    "        hat_Y = self.decision_tree().predict(self.X_test)\n",
    "        hat_Y = hat_Y.reshape(hat_Y.shape[0], 1)\n",
    "        return self.decision_rule(hat_Y)\n",
    "\n",
    "    def correct_rate(self, hat_Y):\n",
    "        return accuracy_score(self.Y_test, hat_Y)\n",
    "    \n",
    "    def test_error(self, hat_Y):\n",
    "        return hamming_loss(self.Y_test, hat_Y)\n",
    "    \n",
    "    def result_report(self, hat_Y):\n",
    "        target_names = ['Irrevelence', 'Relevance']\n",
    "        return classification_report(self.Y_test, hat_Y, target_names=target_names)\n",
    "    \n",
    "    def pca_graph(self, dimension):\n",
    "        #X = laplacian_kernel(self.x_matrix)\n",
    "        #X = polynomial_kernel(self.x_matrix)\n",
    "        X = self.x_matrix\n",
    "        #X = polynomial_kernel(X)\n",
    "        if dimension == 2:\n",
    "            pca = decomposition.PCA(n_components=2)\n",
    "            pca.fit(X)\n",
    "            X = np.array(pca.transform(X))\n",
    "            data_fig1 = plt.figure(1, figsize=(8, 6))\n",
    "            plt.clf()\n",
    "            #Plot the training points\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=np.asarray(self.label_y), cmap=plt.cm.Paired)\n",
    "            plt.xlabel('Projection Vector 1')\n",
    "            plt.ylabel('Projection Vector 2')\n",
    "            plt.show()\n",
    "            data_fig1.savefig('data_2d.png')\n",
    "        elif dimension == 3:\n",
    "            pca = decomposition.PCA(n_components=3)\n",
    "            pca.fit(self.x_matrix)\n",
    "            X = np.array(pca.transform(self.x_matrix))\n",
    "            \n",
    "            data_fig2 = plt.figure(2)\n",
    "            ax2 = data_fig2.add_subplot(111, projection='3d')\n",
    "            ax2.scatter(np.asarray(X[:,0]), np.asarray(X[:,1]), \n",
    "                        np.asarray(X[:, 2]), c=np.asarray(self.label_y), cmap=plt.cm.Paired)\n",
    "            plt.show()\n",
    "            data_fig2.savefig('data_3d.png')\n",
    "            \n",
    "    def __init__(self):\n",
    "        feature_file = 'dataset/women.txt'\n",
    "        label_file = 'dataset/women_y1.txt'\n",
    "        x_matrix = np.loadtxt(feature_file, dtype='float')\n",
    "        label_y = np.loadtxt(label_file, dtype='float')\n",
    "        self.x_matrix = np.matrix(x_matrix)\n",
    "        self.label_y = np.matrix(label_y).T\n",
    "        \n",
    "        #data_matrix = np.concatenate((x_matrix, label_y), axis=1)\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.x_matrix, \n",
    "                                                                                self.label_y, test_size=.4)\n",
    "\n",
    "#############################################################\n",
    "def classification_result():\n",
    "    instance = classifier()\n",
    "    ridge_reg_hatY = instance.predict_ridge_regression()\n",
    "    knn_hatY = instance.predict_knn()\n",
    "    svm_hatY = instance.predict_linearsvm()\n",
    "    bayes_hatY = instance.predict_bayesgaussin()\n",
    "    logistic_hatY = instance.predict_logistic()\n",
    "    lda_hatY = instance.predict_LDA()\n",
    "    lasso_hatY = instance.predict_lasso()\n",
    "    desTree_hatY = instance.predict_decisionTree()\n",
    "\n",
    "    print 'ridge regression'\n",
    "    print instance.result_report(ridge_reg_hatY)\n",
    "\n",
    "    print 'knn'\n",
    "    print instance.result_report(knn_hatY)\n",
    "\n",
    "    print 'linear support vector machine'\n",
    "    print instance.result_report(svm_hatY)\n",
    "\n",
    "    print 'bayes gaussian'\n",
    "    print instance.result_report(bayes_hatY)\n",
    "\n",
    "    print 'logistic regression'\n",
    "    print instance.result_report(logistic_hatY)\n",
    "\n",
    "    print 'LDA'\n",
    "    print instance.result_report(lda_hatY)\n",
    "\n",
    "    print 'lasso'\n",
    "    print instance.result_report(lasso_hatY)\n",
    "\n",
    "    print 'decision tree'\n",
    "    print instance.result_report(desTree_hatY)\n",
    "    \n",
    "###################################################################################\n",
    "query = ['news','season','sports','team','NBA','women','schedule','college','NCAA','player']\n",
    "\n",
    "engine = se('All/basketball/graph/adj_list','All/basketball/graph/nodes','basketball')\n",
    "relevance_pbject = page_relevance(engine)\n",
    "test = relevance_pbject.print_scores()\n",
    "classification_result()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
